{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96a3607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0219eb3c958541e79937e00888aa67d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abc7dbd6beb4d7a9f1b411a004c8d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a099241007144b5ea9afadf08cb5fb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/32 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06126694ffb74271bda5c47522f4d0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/78822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7d8961570947b8b73c6747dc28d047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3871 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e3704fa54f4722b8602484956d34dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /scratch/zdiao7/OLIVES_Dataset\n",
      "Number of samples in the dataset: 78822\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "scratch_path = '/scratch/zdiao7/OLIVES_Dataset'\n",
    "os.makedirs(scratch_path, exist_ok=True)\n",
    "\n",
    "olives = load_dataset('gOLIVES/OLIVES_Dataset', 'biomarker_detection', split='train',cache_dir=scratch_path)\n",
    "\n",
    "print(f\"Dataset downloaded to: {scratch_path}\")\n",
    "print(f\"Number of samples in the dataset: {len(olives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8abed997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "olives = olives.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0636c921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e2205f39994d6fa8374617321eee78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/78822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 17591 samples.\n"
     ]
    }
   ],
   "source": [
    "olives = olives.filter(\n",
    "    lambda x: not torch.isnan(x['B1']) \n",
    "    and not torch.isnan(x['B3']) \n",
    "    and not torch.isnan(x['B4']) \n",
    "    and not torch.isnan(x['B5'])\n",
    ")\n",
    "print(f\"Filtered dataset contains {len(olives)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c0422f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3ca24f61dd4e05ba6c1b1eb2af8fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17591 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/1000067/ipykernel_2659578/2189241773.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  and not torch.isnan(torch.tensor(data_point['BCVA']))\n",
      "/scratch/1000067/ipykernel_2659578/2189241773.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  and not torch.isnan(torch.tensor(data_point['CST']))\n",
      "/scratch/1000067/ipykernel_2659578/2189241773.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  and not any(torch.isnan(torch.tensor(data_point[key])) for key in ['B1', 'B3', 'B4', 'B5'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 17444 samples.\n"
     ]
    }
   ],
   "source": [
    "def is_valid_sample(data_point):\n",
    "    return (\n",
    "        data_point['Image'] is not None\n",
    "        and not torch.isnan(torch.tensor(data_point['BCVA']))\n",
    "        and not torch.isnan(torch.tensor(data_point['CST']))\n",
    "        and not any(torch.isnan(torch.tensor(data_point[key])) for key in ['B1', 'B3', 'B4', 'B5'])\n",
    "    )\n",
    "\n",
    "olives = olives.filter(is_valid_sample)\n",
    "print(f\"Filtered dataset contains {len(olives)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d941042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OlivesDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]['Image'].float()\n",
    "        if len(img.shape) == 3 and img.shape[0] == 3:\n",
    "            img = img.mean(dim=0, keepdim=True)\n",
    "        elif len(img.shape) == 2:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        clinical = torch.tensor([self.data[idx]['BCVA'], self.data[idx]['CST']], dtype=torch.float32)\n",
    "        label = torch.tensor(\n",
    "            [self.data[idx]['B1'], self.data[idx]['B3'], self.data[idx]['B4'], self.data[idx]['B5']],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        return {'Image': img, 'Clinical': clinical, 'Label': label}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ada4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "train_dataset = OlivesDataset(olives, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78510909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g = F.interpolate(g, size=x.shape[2:], mode='bilinear', align_corners=True)\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=256):\n",
    "        super(AttentionUNet, self).__init__()\n",
    "        self.enc1 = self.conv_block(input_channels, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        self.middle = self.conv_block(512, 1024)\n",
    "\n",
    "        self.att4 = AttentionBlock(1024, 512, 256)\n",
    "        self.att3 = AttentionBlock(512, 256, 128)\n",
    "        self.att2 = AttentionBlock(256, 128, 64)\n",
    "\n",
    "        self.dec4 = self.conv_block(1024 + 512, 512)\n",
    "        self.dec3 = self.conv_block(512 + 256, 256)\n",
    "        self.dec2 = self.conv_block(256 + 128, 128)\n",
    "        self.dec1 = self.conv_block(128 + 64, 64)\n",
    "\n",
    "        self.output_conv = nn.Conv2d(64, output_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(F.max_pool2d(enc1, 2))\n",
    "        enc3 = self.enc3(F.max_pool2d(enc2, 2))\n",
    "        enc4 = self.enc4(F.max_pool2d(enc3, 2))\n",
    "        middle = self.middle(F.max_pool2d(enc4, 2))\n",
    "\n",
    "        att4 = self.att4(middle, enc4)\n",
    "        dec4 = self.dec4(torch.cat([F.interpolate(middle, size=enc4.shape[2:], mode='bilinear', align_corners=True), att4], dim=1))\n",
    "\n",
    "        att3 = self.att3(dec4, enc3)\n",
    "        dec3 = self.dec3(torch.cat([F.interpolate(dec4, size=enc3.shape[2:], mode='bilinear', align_corners=True), att3], dim=1))\n",
    "\n",
    "        att2 = self.att2(dec3, enc2)\n",
    "        dec2 = self.dec2(torch.cat([F.interpolate(dec3, size=enc2.shape[2:], mode='bilinear', align_corners=True), att2], dim=1))\n",
    "\n",
    "        dec1 = self.dec1(torch.cat([F.interpolate(dec2, size=enc1.shape[2:], mode='bilinear', align_corners=True), enc1], dim=1))\n",
    "        outputs = self.output_conv(dec1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0de617e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiAttentionUNet, self).__init__()\n",
    "        self.image_model = AttentionUNet(input_channels=1, output_channels=256)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.clinical_fc = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16)\n",
    "        )\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(256 + 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, clinical_features):\n",
    "        image_features = self.image_model(images)\n",
    "        image_features = self.global_pool(image_features).view(image_features.size(0), -1)\n",
    "        clinical_features = self.clinical_fc(clinical_features)\n",
    "        combined_features = torch.cat((image_features, clinical_features), dim=1)\n",
    "        outputs = self.fusion_fc(combined_features)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17beeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MultiAttentionUNet().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.4)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=10, save_path=\"trained_attention_unet.pth\"):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            images = batch['Image'].to(device)\n",
    "            clinical = batch['Clinical'].to(device)\n",
    "            labels = batch['Label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, clinical)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eb86b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.5099\n",
      "Epoch 2/40, Loss: 0.3966\n",
      "Epoch 3/40, Loss: 0.3446\n",
      "Epoch 4/40, Loss: 0.3169\n",
      "Epoch 5/40, Loss: 0.2790\n",
      "Epoch 6/40, Loss: 0.2445\n",
      "Epoch 7/40, Loss: 0.1936\n",
      "Epoch 8/40, Loss: 0.1663\n",
      "Epoch 9/40, Loss: 0.1436\n",
      "Epoch 10/40, Loss: 0.1130\n",
      "Epoch 11/40, Loss: 0.0878\n",
      "Epoch 12/40, Loss: 0.0629\n",
      "Epoch 13/40, Loss: 0.0316\n",
      "Epoch 14/40, Loss: 0.0239\n",
      "Epoch 15/40, Loss: 0.0192\n",
      "Epoch 16/40, Loss: 0.0147\n",
      "Epoch 17/40, Loss: 0.0136\n",
      "Epoch 18/40, Loss: 0.0122\n",
      "Epoch 19/40, Loss: 0.0070\n",
      "Epoch 20/40, Loss: 0.0059\n",
      "Epoch 21/40, Loss: 0.0060\n",
      "Epoch 22/40, Loss: 0.0062\n",
      "Epoch 23/40, Loss: 0.0044\n",
      "Epoch 24/40, Loss: 0.0042\n",
      "Epoch 25/40, Loss: 0.0037\n",
      "Epoch 26/40, Loss: 0.0032\n",
      "Epoch 27/40, Loss: 0.0028\n",
      "Epoch 28/40, Loss: 0.0030\n",
      "Epoch 29/40, Loss: 0.0026\n",
      "Epoch 30/40, Loss: 0.0025\n",
      "Epoch 31/40, Loss: 0.0023\n",
      "Epoch 32/40, Loss: 0.0025\n",
      "Epoch 33/40, Loss: 0.0022\n",
      "Epoch 34/40, Loss: 0.0021\n",
      "Epoch 35/40, Loss: 0.0021\n",
      "Epoch 36/40, Loss: 0.0028\n",
      "Epoch 37/40, Loss: 0.0020\n",
      "Epoch 38/40, Loss: 0.0021\n",
      "Epoch 39/40, Loss: 0.0018\n",
      "Epoch 40/40, Loss: 0.0018\n",
      "Model saved to trained_attention_unet.pth\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, scheduler, epochs=40, save_path=\"trained_attention_unet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b88579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74741e1710aa43d1ae32473c4dd59a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f9cedba9a740b7ab97af7d49a412cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Micro): 0.7143\n",
      "F1 Score (Macro): 0.6514\n",
      "F1 Score (Biomarker 1): 0.6579\n",
      "F1 Score (Biomarker 2): 0.8105\n",
      "F1 Score (Biomarker 3): 0.5657\n",
      "F1 Score (Biomarker 4): 0.5714\n",
      "ROC-AUC (Micro): 0.8905\n",
      "ROC-AUC (Macro): 0.8572\n",
      "ROC-AUC (Biomarker 1): 0.8312\n",
      "ROC-AUC (Biomarker 2): 0.8088\n",
      "ROC-AUC (Biomarker 3): 0.8436\n",
      "ROC-AUC (Biomarker 4): 0.9455\n"
     ]
    }
   ],
   "source": [
    "test_olives = load_dataset('gOLIVES/OLIVES_Dataset', 'biomarker_detection', split='test',cache_dir=scratch_path)\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "test_olives = test_olives.with_format(\"torch\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = MultiAttentionUNet().to(device)\n",
    "model.load_state_dict(torch.load(\"trained_attention_unet.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_dataset = OlivesDataset(test_olives, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def evaluate_f1_and_roc_auc(model, data_loader, threshold=0.85):\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch['Image'].to(device)\n",
    "            clinical = batch['Clinical'].to(device)\n",
    "            labels = batch['Label'].to(device)\n",
    "\n",
    "            outputs = model(images, clinical)\n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "            predictions = (probabilities > threshold).astype(int)\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_predictions.append(predictions)\n",
    "            all_probabilities.append(probabilities)\n",
    "\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_probabilities = np.vstack(all_probabilities)\n",
    "\n",
    "    f1_micro = f1_score(all_labels, all_predictions, average='micro')\n",
    "    f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
    "    per_biomarker_f1 = [\n",
    "        f1_score(all_labels[:, i], all_predictions[:, i], average='binary')\n",
    "        for i in range(all_labels.shape[1])\n",
    "    ]\n",
    "\n",
    "    roc_auc_micro = roc_auc_score(all_labels, all_probabilities, average='micro')\n",
    "    roc_auc_macro = roc_auc_score(all_labels, all_probabilities, average='macro')\n",
    "    per_biomarker_roc_auc = [\n",
    "        roc_auc_score(all_labels[:, i], all_probabilities[:, i])\n",
    "        for i in range(all_labels.shape[1])\n",
    "    ]\n",
    "\n",
    "    print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    for i, f1 in enumerate(per_biomarker_f1):\n",
    "        print(f\"F1 Score (Biomarker {i + 1}): {f1:.4f}\")\n",
    "\n",
    "    print(f\"ROC-AUC (Micro): {roc_auc_micro:.4f}\")\n",
    "    print(f\"ROC-AUC (Macro): {roc_auc_macro:.4f}\")\n",
    "    for i, auc_score in enumerate(per_biomarker_roc_auc):\n",
    "        print(f\"ROC-AUC (Biomarker {i + 1}): {auc_score:.4f}\")\n",
    "\n",
    "    return f1_micro, f1_macro, per_biomarker_f1, roc_auc_micro, roc_auc_macro, per_biomarker_roc_auc\n",
    "\n",
    "f1_micro, f1_macro, per_biomarker_f1, roc_auc_micro, roc_auc_macro, per_biomarker_roc_auc = evaluate_f1_and_roc_auc(\n",
    "    model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca6f95ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biomarker 1 Accuracy: 76.49%\n",
      "Biomarker 2 Accuracy: 78.38%\n",
      "Biomarker 3 Accuracy: 82.51%\n",
      "Biomarker 4 Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(model, test_dataset, threshold=0.85):\n",
    "\n",
    "    model.eval()\n",
    "    biomarker_correct_counts = np.zeros(4) \n",
    "    biomarker_total_counts = np.zeros(4)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(test_dataset)):\n",
    "            sample = test_dataset[idx]\n",
    "            img = sample['Image'].unsqueeze(0).to(device)\n",
    "            clinical = sample['Clinical'].unsqueeze(0).to(device)\n",
    "            label = sample['Label'].numpy().astype(int)\n",
    "\n",
    "            output = model(img, clinical)\n",
    "            prediction = torch.sigmoid(output).squeeze(0).cpu().numpy()\n",
    "\n",
    "            binary_prediction = (prediction > threshold).astype(int)\n",
    "\n",
    "            for i in range(4):\n",
    "                biomarker_total_counts[i] += 1\n",
    "                if binary_prediction[i] == label[i]:\n",
    "                    biomarker_correct_counts[i] += 1\n",
    "\n",
    "    per_biomarker_accuracy = biomarker_correct_counts / biomarker_total_counts\n",
    "\n",
    "    for i, accuracy in enumerate(per_biomarker_accuracy):\n",
    "        print(f\"Biomarker {i + 1} Accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    return per_biomarker_accuracy\n",
    "\n",
    "per_biomarker_accuracy = evaluate_accuracy(model, test_dataset, threshold=0.85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80392651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
